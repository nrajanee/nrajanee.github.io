---
permalink: /
title: #"About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am completing the Advanced Master’s Research Program at Columbia University focused in AI/ML track advised by Prof. Richard Zemel. My research focuses on AI robustness and safety, particularly in multi-modal large language models and their applications in fields such as healthcare. 

After graduation, (in May 2025), I want to contribute to building more trustworthy and resilient AI systems. I am open to opportunities in applied AI research and engineering.

Here's my [CV](/files/Nikita_Rajaneesh_CV.pdf) . 

## Research

### Towards effective discrimination testing for generative AI
Thomas P Zollo, **Nikita Rajaneesh**, Richard Zemel, Talia B. Gillis, and Emily Black. Towards effective
discrimination testing for generative AI. In ICLR 2025 Workshop on Building Trust in Language Models
and Applications, 2025. 

[Full Paper:](https://arxiv.org/abs/2412.21052)

[Github:](https://github.com/thomaspzollo/dhacking)

Generative AI (GenAI) models present new challenges in regulating against discriminatory behavior. In this paper, we argue that GenAI fairness research still has not met these challenges; instead, a significant gap remains between existing bias assessment methods and regulatory goals. Through four case studies, we demonstrate how this misalignment between fairness testing techniques and regulatory goals can result in discriminatory outcomes in real-world deployments, especially in adaptive or complex environments. We offer practical recommendations for improving discrimination testing to better align with regulatory goals and enhance the reliability of fairness assessments in future deployments.



![Red teaming image](/images/red_teaming.png)

![Multi-turn image](/images/multi_turn.png)

### Unsupervised Domain Adaptation for Multimodal Large Language Models.

In collaboration with Thomas Zollo and under the supervision of Richard Zemel. 

[Paper:] Coming soon

[Github:] Coming soon

We propose a novel unsupervised domain adaptation approach for
improving generalization in Multimodal large language models. Instead of relying on extensive labeled datasets, our method leverages a weakly supervised auxiliary task to guide adaptation, allowing the model to refine its representations in new domains
without requiring large amounts of high-quality annotations. Our algorithm aims to preserve the model’s rich pre-training knowledge while enhancing its ability to generalize to diverse, real-world tasks. 

**Results so far**: Our method has shown a 5.6% improvement in accuracy in a visual question answering dataset [GQA](https://cs.stanford.edu/people/dorarad/gqa/about.html) dataset and a 2.6% improvement in accuracy in a medical dataset [VQA-Rad](https://paperswithcode.com/dataset/vqa-rad).

## Work Experience 

### Software Engineer, (AI/ML), Determined AI 

### Software Engineer, Morningstar 